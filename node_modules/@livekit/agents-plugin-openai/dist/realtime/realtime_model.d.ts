import { AsyncIterableQueue, Future, llm, multimodal } from '@livekit/agents';
import { AudioFrame } from '@livekit/rtc-node';
import * as api_proto from './api_proto.js';
interface ModelOptions {
    modalities: ['text', 'audio'] | ['text'];
    instructions: string;
    voice: api_proto.Voice;
    inputAudioFormat: api_proto.AudioFormat;
    outputAudioFormat: api_proto.AudioFormat;
    inputAudioTranscription: api_proto.InputAudioTranscription | null;
    turnDetection: api_proto.TurnDetectionType | null;
    temperature: number;
    maxResponseOutputTokens: number;
    model: api_proto.Model;
    apiKey?: string;
    baseURL: string;
    isAzure: boolean;
    entraToken?: string;
    apiVersion?: string;
}
export interface RealtimeResponse {
    id: string;
    status: api_proto.ResponseStatus;
    statusDetails: api_proto.ResponseStatusDetails | null;
    usage: api_proto.ModelUsage | null;
    output: RealtimeOutput[];
    doneFut: Future;
    createdTimestamp: number;
    firstTokenTimestamp?: number;
}
export interface RealtimeOutput {
    responseId: string;
    itemId: string;
    outputIndex: number;
    role: api_proto.Role;
    type: 'message' | 'function_call';
    content: RealtimeContent[];
    doneFut: Future;
}
export interface RealtimeContent {
    responseId: string;
    itemId: string;
    outputIndex: number;
    contentIndex: number;
    text: string;
    audio: AudioFrame[];
    textStream: AsyncIterableQueue<string>;
    audioStream: AsyncIterableQueue<AudioFrame>;
    toolCalls: RealtimeToolCall[];
    contentType: api_proto.Modality;
}
export interface RealtimeToolCall {
    name: string;
    arguments: string;
    toolCallID: string;
}
export interface InputSpeechTranscriptionCompleted {
    itemId: string;
    transcript: string;
}
export interface InputSpeechTranscriptionFailed {
    itemId: string;
    message: string;
}
export interface InputSpeechStarted {
    itemId: string;
}
export interface InputSpeechCommitted {
    itemId: string;
}
declare class InputAudioBuffer {
    #private;
    constructor(session: RealtimeSession);
    append(frame: AudioFrame): void;
    clear(): void;
    commit(): void;
}
declare class ConversationItem {
    #private;
    constructor(session: RealtimeSession);
    truncate(itemId: string, contentIndex: number, audioEnd: number): void;
    delete(itemId: string): void;
    create(message: llm.ChatMessage, previousItemId?: string): void;
}
declare class Conversation {
    #private;
    constructor(session: RealtimeSession);
    get item(): ConversationItem;
}
declare class Response {
    #private;
    constructor(session: RealtimeSession);
    create(): void;
    cancel(): void;
}
export declare class RealtimeModel extends multimodal.RealtimeModel {
    #private;
    sampleRate: number;
    numChannels: number;
    inFrameSize: number;
    outFrameSize: number;
    static withAzure({ baseURL, azureDeployment, apiVersion, apiKey, entraToken, instructions, modalities, voice, inputAudioFormat, outputAudioFormat, inputAudioTranscription, turnDetection, temperature, maxResponseOutputTokens, }: {
        baseURL: string;
        azureDeployment: string;
        apiVersion?: string;
        apiKey?: string;
        entraToken?: string;
        instructions?: string;
        modalities?: ['text', 'audio'] | ['text'];
        voice?: api_proto.Voice;
        inputAudioFormat?: api_proto.AudioFormat;
        outputAudioFormat?: api_proto.AudioFormat;
        inputAudioTranscription?: api_proto.InputAudioTranscription;
        turnDetection?: api_proto.TurnDetectionType;
        temperature?: number;
        maxResponseOutputTokens?: number;
    }): RealtimeModel;
    constructor({ modalities, instructions, voice, inputAudioFormat, outputAudioFormat, inputAudioTranscription, turnDetection, temperature, maxResponseOutputTokens, model, apiKey, baseURL, isAzure, apiVersion, entraToken, }: {
        modalities?: ['text', 'audio'] | ['text'];
        instructions?: string;
        voice?: api_proto.Voice;
        inputAudioFormat?: api_proto.AudioFormat;
        outputAudioFormat?: api_proto.AudioFormat;
        inputAudioTranscription?: api_proto.InputAudioTranscription;
        turnDetection?: api_proto.TurnDetectionType;
        temperature?: number;
        maxResponseOutputTokens?: number;
        model?: api_proto.Model;
        apiKey?: string;
        baseURL?: string;
        isAzure?: boolean;
        apiVersion?: string;
        entraToken?: string;
    });
    get sessions(): RealtimeSession[];
    session({ fncCtx, chatCtx, modalities, instructions, voice, inputAudioFormat, outputAudioFormat, inputAudioTranscription, turnDetection, temperature, maxResponseOutputTokens, }: {
        fncCtx?: llm.FunctionContext;
        chatCtx?: llm.ChatContext;
        modalities?: ['text', 'audio'] | ['text'];
        instructions?: string;
        voice?: api_proto.Voice;
        inputAudioFormat?: api_proto.AudioFormat;
        outputAudioFormat?: api_proto.AudioFormat;
        inputAudioTranscription?: api_proto.InputAudioTranscription | null;
        turnDetection?: api_proto.TurnDetectionType | null;
        temperature?: number;
        maxResponseOutputTokens?: number;
    }): RealtimeSession;
    close(): Promise<void>;
}
export declare class RealtimeSession extends multimodal.RealtimeSession {
    #private;
    constructor(opts: ModelOptions, { fncCtx, chatCtx }: {
        fncCtx?: llm.FunctionContext;
        chatCtx?: llm.ChatContext;
    });
    get chatCtx(): llm.ChatContext | undefined;
    get fncCtx(): llm.FunctionContext | undefined;
    set fncCtx(ctx: llm.FunctionContext | undefined);
    get conversation(): Conversation;
    get inputAudioBuffer(): InputAudioBuffer;
    get response(): Response;
    get expiration(): number;
    queueMsg(command: api_proto.ClientEvent): void;
    sessionUpdate({ modalities, instructions, voice, inputAudioFormat, outputAudioFormat, inputAudioTranscription, turnDetection, temperature, maxResponseOutputTokens, toolChoice, selectedTools, }: {
        modalities: ['text', 'audio'] | ['text'];
        instructions?: string;
        voice?: api_proto.Voice;
        inputAudioFormat?: api_proto.AudioFormat;
        outputAudioFormat?: api_proto.AudioFormat;
        inputAudioTranscription?: api_proto.InputAudioTranscription | null;
        turnDetection?: api_proto.TurnDetectionType | null;
        temperature?: number;
        maxResponseOutputTokens?: number;
        toolChoice?: api_proto.ToolChoice;
        selectedTools?: string[];
    }): void;
    /**
     * Try to recover from a text response to audio mode.
     *
     * @remarks
     * Sometimes the OpenAI Realtime API returns text instead of audio responses.
     * This method tries to recover from this by requesting a new response after deleting the text
     * response and creating an empty user audio message.
     */
    recoverFromTextResponse(itemId: string): void;
    close(): Promise<void>;
}
export {};
//# sourceMappingURL=realtime_model.d.ts.map